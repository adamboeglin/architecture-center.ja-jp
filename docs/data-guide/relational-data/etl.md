---
title: 抽出、変換、読み込み (ETL)
description: ''
author: zoinerTejada
ms.date: 02/12/2018
ms.topic: guide
ms.service: architecture-center
ms.subservice: cloud-fundamentals
ms.openlocfilehash: aa578464947e51964fee9859395149b44b47fa00
ms.sourcegitcommit: 579c39ff4b776704ead17a006bf24cd4cdc65edd
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 04/17/2019
ms.locfileid: "59639990"
---
# <a name="extract-transform-and-load-etl"></a><span data-ttu-id="d9fd7-102">抽出、変換、読み込み (ETL)</span><span class="sxs-lookup"><span data-stu-id="d9fd7-102">Extract, transform, and load (ETL)</span></span>

<span data-ttu-id="d9fd7-103">組織が直面する一般的な問題は、複数のソースから複数の形式でデータを収集し、1 つまたは複数のデータ ストアに移動する方法です。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-103">A common problem that organizations face is how to gathering data from multiple sources, in multiple formats, and move it to one or more data stores.</span></span> <span data-ttu-id="d9fd7-104">移動先がソースと同じ種類のデータ ストアではないことがあり、多くの場合、形式が異なったり、最終的な宛先に読み込む前にデータの成形やクリーニングが必要だったりします。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-104">The destination may not be the same type of data store as the source, and often the format is different, or the data needs to be shaped or cleaned before loading it into its final destination.</span></span>

<span data-ttu-id="d9fd7-105">こうした課題に対処するために、長年にわたってさまざまなツール、サービス、プロセスが開発されてきました。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-105">Various tools, services, and processes have been developed over the years to help address these challenges.</span></span> <span data-ttu-id="d9fd7-106">使用するプロセスに関係なく、データ パイプライン内で作業を調整し、一定レベルのデータ変換を適用するという共通のニーズがあります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-106">No matter the process used, there is a common need to coordinate the work and apply some level of data transformation within the data pipeline.</span></span> <span data-ttu-id="d9fd7-107">以下のセクションでは、これらのタスクを実行するために使用する一般的な手法に注目します。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-107">The following sections highlight the common methods used to perform these tasks.</span></span>

## <a name="extract-transform-and-load-etl-process"></a><span data-ttu-id="d9fd7-108">抽出、変換、および読み込み (ETL) プロセス</span><span class="sxs-lookup"><span data-stu-id="d9fd7-108">Extract, transform, and load (ETL) process</span></span>

<span data-ttu-id="d9fd7-109">抽出、変換、読み込み (ETL) は、さまざまなソースからデータを収集し、ビジネス ルールに従ってデータを変換して、宛先データ ストアに読み込むために使用されるデータ パイプラインです。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-109">Extract, transform, and load (ETL) is a data pipeline used to collect data from various sources, transform the data according to business rules, and load it into a destination data store.</span></span> <span data-ttu-id="d9fd7-110">ETL の変換作業は特殊なエンジンで行われ、多くの場合、変換されて最終的に宛先に読み込まれるデータの一時的な保持にステージング テーブルを使用します。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-110">The transformation work in ETL takes place in a specialized engine, and often involves using staging tables to temporarily hold data as it is being transformed and ultimately loaded to its destination.</span></span>

<span data-ttu-id="d9fd7-111">通常、実行されるデータ変換には、フィルター処理、並べ替え、集計、データの結合、データのクリーニング、重複除去、データの検証などのさまざまな操作が含まれます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-111">The data transformation that takes place usually involves various operations, such as filtering, sorting, aggregating, joining data, cleaning data, deduplicating, and validating data.</span></span>

![抽出、変換、読み込み (ETL) プロセス](../images/etl.png)

<span data-ttu-id="d9fd7-113">多くの場合、時間を節約するために 3 つの ETL フェーズが並列に実行されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-113">Often, the three ETL phases are run in parallel to save time.</span></span> <span data-ttu-id="d9fd7-114">たとえば、データが抽出されている間、変換プロセスは既に受信したデータを操作して読み込みの準備を行うことができ、読み込みプロセスは抽出プロセス全体が完了するまで待たずに準備されたデータの処理を開始できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-114">For example, while data is being extracted, a transformation process could be working on data already received and prepare it for loading, and a loading process can begin working on the prepared data, rather than waiting for the entire extraction process to complete.</span></span>

<span data-ttu-id="d9fd7-115">関連 Azure サービス:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-115">Relevant Azure service:</span></span>

- [<span data-ttu-id="d9fd7-116">Azure Data Factory V2</span><span class="sxs-lookup"><span data-stu-id="d9fd7-116">Azure Data Factory v2</span></span>](https://azure.microsoft.com/services/data-factory/)

<span data-ttu-id="d9fd7-117">その他のツール:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-117">Other tools:</span></span>

- [<span data-ttu-id="d9fd7-118">SQL Server Integration Services (SSIS)</span><span class="sxs-lookup"><span data-stu-id="d9fd7-118">SQL Server Integration Services (SSIS)</span></span>](/sql/integration-services/sql-server-integration-services)

## <a name="extract-load-and-transform-elt"></a><span data-ttu-id="d9fd7-119">抽出、読み込み、変換 (ELT)</span><span class="sxs-lookup"><span data-stu-id="d9fd7-119">Extract, load, and transform (ELT)</span></span>

<span data-ttu-id="d9fd7-120">抽出、読み込み、変換 (ELT) は、変換が行われる場所だけが ETL と異なります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-120">Extract, load, and transform (ELT) differs from ETL solely in where the transformation takes place.</span></span> <span data-ttu-id="d9fd7-121">ELT パイプラインでは、変換はターゲット データ ストアで行われます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-121">In the ELT pipeline, the transformation occurs in the target data store.</span></span> <span data-ttu-id="d9fd7-122">独立した変換エンジンを使用する代わりに、ターゲット データ ストアの処理機能がデータ変換に使用されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-122">Instead of using a separate transformation engine, the processing capabilities of the target data store are used to transform data.</span></span> <span data-ttu-id="d9fd7-123">これにより、パイプラインから変換エンジンが除去されるためアーキテクチャがシンプルになります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-123">This simplifies the architecture by removing the transformation engine from the pipeline.</span></span> <span data-ttu-id="d9fd7-124">このアプローチのもう 1 つの利点は、ターゲット データ ストアをスケーリングすると ELT パイプラインのパフォーマンスもスケーリングされることです。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-124">Another benefit to this approach is that scaling the target data store also scales the ELT pipeline performance.</span></span> <span data-ttu-id="d9fd7-125">ただし、ELT が効果的に機能するのは、ターゲット システムが十分に強力でデータを効率的に変換できる場合だけです。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-125">However, ELT only works well when the target system is powerful enough to transform the data efficiently.</span></span>

![抽出、読み込み、変換 (ELT) プロセス](../images/elt.png)

<span data-ttu-id="d9fd7-127">ELT の一般的なユース ケースは、ビッグ データ領域に分類されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-127">Typical use cases for ELT fall within the big data realm.</span></span> <span data-ttu-id="d9fd7-128">たとえば、すべてのソース データを Hadoop 分散ファイル システム (HDFS) や Azure Data Lake Store などのスケーラブルなストレージのフラット ファイルに抽出することから開始できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-128">For example, you might start by extracting all of the source data to flat files in scalable storage such as Hadoop distributed file system (HDFS) or Azure Data Lake Store.</span></span> <span data-ttu-id="d9fd7-129">Spark、Hive、PolyBase などのテクノロジを使用して、ソース データのクエリを実行できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-129">Technologies such as Spark, Hive, or PolyBase can then be used to query the source data.</span></span> <span data-ttu-id="d9fd7-130">ELT で重要な点は、変換を実行するために使用するデータ ストアと、データが最終的に使用されるデータ ストアが同じであることです。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-130">The key point with ELT is that the data store used to perform the transformation is the same data store where the data is ultimately consumed.</span></span> <span data-ttu-id="d9fd7-131">このデータ ストアは、専用ストレージにデータを読み込むのではなく、スケーラブルなストレージから直接読み取ります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-131">This data store reads directly from the scalable storage, instead of loading the data into its own proprietary storage.</span></span> <span data-ttu-id="d9fd7-132">このアプローチは、大規模なデータ セットでは時間がかかる操作である ETL のデータ コピー ステップをスキップします。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-132">This approach skips the data copy step present in ETL, which can be a time consuming operation for large data sets.</span></span>

<span data-ttu-id="d9fd7-133">実際には、ターゲット データ ストアは、Hadoop クラスター (Hive または Spark を使用) または SQL Data Warehouse を使用した[データ ウェアハウス](./data-warehousing.md)です。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-133">In practice, the target data store is a [data warehouse](./data-warehousing.md) using either a Hadoop cluster (using Hive or Spark) or a SQL Data Warehouse.</span></span> <span data-ttu-id="d9fd7-134">一般に、スキーマはクエリ時にフラット ファイル データにオーバーレイされ、テーブルとして格納されるので、データ ストア内の他のテーブルと同じようにデータのクエリを実行できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-134">In general, a schema is overlaid on the flat file data at query time and stored as a table, enabling the data to be queried like any other table in the data store.</span></span> <span data-ttu-id="d9fd7-135">これらは、データがデータ ストア自体によって管理されるストレージに存在せず、外部のスケーラブルなストレージに存在するため、外部テーブルと呼ばれます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-135">These are referred to as external tables because the data does not reside in storage managed by the data store itself, but on some external scalable storage.</span></span>

<span data-ttu-id="d9fd7-136">データ ストアでは、データのスキーマのみが管理され、読み取り時にスキーマが適用されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-136">The data store only manages the schema of the data and applies the schema on read.</span></span> <span data-ttu-id="d9fd7-137">たとえば、Hive を使用した Hadoop クラスターでは、データ ソースが実質的に HDFS のファイル セットへのパスである Hive テーブルが記述されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-137">For example, a Hadoop cluster using Hive would describe a Hive table where the data source is effectively a path to a set of files in HDFS.</span></span> <span data-ttu-id="d9fd7-138">SQL Data Warehouse では、PolyBase で同じ結果を得ることができます。つまり、データベース自体に外部的に格納されるデータに対してテーブルが作成されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-138">In SQL Data Warehouse, PolyBase can achieve the same result &mdash; creating a table against data stored externally to the database itself.</span></span> <span data-ttu-id="d9fd7-139">ソース データが読み込まれると、データ ストアの機能を使用して外部テーブル内のデータを処理できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-139">Once the source data is loaded, the data present in the external tables can be processed using the capabilities of the data store.</span></span> <span data-ttu-id="d9fd7-140">このため、ビッグ データのシナリオでは、データ ストアが超並列処理 (MPP) に対応している必要があります。超並列処理では、データが小さなチャンクに分割され、複数のマシン間でチャンクの処理が並列に分散されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-140">In big data scenarios, this means the data store must be capable of massively parallel processing (MPP), which breaks the data into smaller chunks and distributes processing of the chunks across multiple machines in parallel.</span></span>

<span data-ttu-id="d9fd7-141">通常、ELT パイプラインの最後のフェーズでは、サポートする必要があるクエリの種類に対してより効率的な最終形式にソース データを変換します。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-141">The final phase of the ELT pipeline is typically to transform the source data into a final format that is more efficient for the types of queries that need to be supported.</span></span> <span data-ttu-id="d9fd7-142">たとえば、データがパーティション分割されることがあります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-142">For example, the data may be partitioned.</span></span> <span data-ttu-id="d9fd7-143">また ELT では、行指向型データを列形式で格納してインデックス作成を最適化する Parquet などの最適化されたストレージ形式を使用できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-143">Also, ELT might use optimized storage formats like Parquet, which stores row-oriented data in a columnar fashion and provides optimized indexing.</span></span>

<span data-ttu-id="d9fd7-144">関連 Azure サービス:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-144">Relevant Azure service:</span></span>

- [<span data-ttu-id="d9fd7-145">Azure SQL Data Warehouse</span><span class="sxs-lookup"><span data-stu-id="d9fd7-145">Azure SQL Data Warehouse</span></span>](/azure/sql-data-warehouse/sql-data-warehouse-overview-what-is)
- [<span data-ttu-id="d9fd7-146">Hive を使用する HDInsight</span><span class="sxs-lookup"><span data-stu-id="d9fd7-146">HDInsight with Hive</span></span>](/azure/hdinsight/hadoop/hdinsight-use-hive)
- [<span data-ttu-id="d9fd7-147">Azure Data Factory V2</span><span class="sxs-lookup"><span data-stu-id="d9fd7-147">Azure Data Factory v2</span></span>](https://azure.microsoft.com/services/data-factory/)
- [<span data-ttu-id="d9fd7-148">HDInsight での Oozie</span><span class="sxs-lookup"><span data-stu-id="d9fd7-148">Oozie on HDInsight</span></span>](/azure/hdinsight/hdinsight-use-oozie-linux-mac)

<span data-ttu-id="d9fd7-149">その他のツール:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-149">Other tools:</span></span>

- [<span data-ttu-id="d9fd7-150">SQL Server Integration Services (SSIS)</span><span class="sxs-lookup"><span data-stu-id="d9fd7-150">SQL Server Integration Services (SSIS)</span></span>](/sql/integration-services/sql-server-integration-services)

## <a name="data-flow-and-control-flow"></a><span data-ttu-id="d9fd7-151">データ フローと制御フロー</span><span class="sxs-lookup"><span data-stu-id="d9fd7-151">Data flow and control flow</span></span>

<span data-ttu-id="d9fd7-152">データ パイプラインのコンテキストでは、制御フローは一連のタスクが適切な順序で処理されるようにします。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-152">In the context of data pipelines, the control flow ensures orderly processing of a set of tasks.</span></span> <span data-ttu-id="d9fd7-153">これらのタスクの適切な処理順序を適用するために、優先順位制約が使用されます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-153">To enforce the correct processing order of these tasks, precedence constraints are used.</span></span> <span data-ttu-id="d9fd7-154">次のイメージに示すように、これらの制約はワークフロー図のコネクタとして考えることができます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-154">You can think of these constraints as connectors in a workflow diagram, as shown in the image below.</span></span> <span data-ttu-id="d9fd7-155">各タスクには、成功、失敗、完了などの結果があります。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-155">Each task has an outcome, such as success, failure, or completion.</span></span> <span data-ttu-id="d9fd7-156">後続タスクは、先行タスクがこれらの結果のいずれかで完了するまで処理を開始しません。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-156">Any subsequent task does not initiate processing until its predecessor has completed with one of these outcomes.</span></span>

<span data-ttu-id="d9fd7-157">制御フローは、データ フローをタスクとして実行します。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-157">Control flows execute data flows as a task.</span></span> <span data-ttu-id="d9fd7-158">データ フロー タスクでは、データはソースから抽出、変換、またはデータ ストアに読み込まれます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-158">In a data flow task, data is extracted from a source, transformed, or loaded into a data store.</span></span> <span data-ttu-id="d9fd7-159">1 つのデータ フロー タスクの出力を次のデータ フロー タスクへの入力にすることができ、データ フローを並列で実行できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-159">The output of one data flow task can be the input to the next data flow task, and data flows can run in parallel.</span></span> <span data-ttu-id="d9fd7-160">制御フローとは異なり、データ フロー内のタスク間に制約を追加することはできません。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-160">Unlike control flows, you cannot add constraints between tasks in a data flow.</span></span> <span data-ttu-id="d9fd7-161">ただし、データ ビューアーを追加して、各タスクによって処理されるデータを監視できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-161">You can, however, add a data viewer to observe the data as it is processed by each task.</span></span>

![制御フロー内でタスクとして実行されるデータ フロー](../images/control-flow-data-flow.png)

<span data-ttu-id="d9fd7-163">上の図では、制御フロー内に複数のタスクがあり、その 1 つはデータ フロー タスクです。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-163">In the diagram above, there are several tasks within the control flow, one of which is a data flow task.</span></span> <span data-ttu-id="d9fd7-164">タスクの 1 つはコンテナー内で入れ子になっています。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-164">One of the tasks is nested within a container.</span></span> <span data-ttu-id="d9fd7-165">コンテナーを使用してタスクに構造体を提供し、作業ユニットを提供できます。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-165">Containers can be used to provide structure to tasks, providing a unit of work.</span></span> <span data-ttu-id="d9fd7-166">このような例の 1 つは、フォルダー内のファイルやデータベース ステートメントなど、コレクション内の要素を繰り返す場合です。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-166">One such example is for repeating elements within a collection, such as files in a folder or database statements.</span></span>

<span data-ttu-id="d9fd7-167">関連 Azure サービス:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-167">Relevant Azure service:</span></span>

- [<span data-ttu-id="d9fd7-168">Azure Data Factory V2</span><span class="sxs-lookup"><span data-stu-id="d9fd7-168">Azure Data Factory v2</span></span>](https://azure.microsoft.com/services/data-factory/)

<span data-ttu-id="d9fd7-169">その他のツール:</span><span class="sxs-lookup"><span data-stu-id="d9fd7-169">Other tools:</span></span>

- [<span data-ttu-id="d9fd7-170">SQL Server Integration Services (SSIS)</span><span class="sxs-lookup"><span data-stu-id="d9fd7-170">SQL Server Integration Services (SSIS)</span></span>](/sql/integration-services/sql-server-integration-services)

## <a name="technology-choices"></a><span data-ttu-id="d9fd7-171">テクノロジの選択</span><span class="sxs-lookup"><span data-stu-id="d9fd7-171">Technology choices</span></span>

- [<span data-ttu-id="d9fd7-172">オンライン トランザクション処理 (OLTP) データ ストア</span><span class="sxs-lookup"><span data-stu-id="d9fd7-172">Online Transaction Processing (OLTP) data stores</span></span>](./online-transaction-processing.md#oltp-in-azure)
- [<span data-ttu-id="d9fd7-173">オンライン分析処理 (OLAP) データ ストア</span><span class="sxs-lookup"><span data-stu-id="d9fd7-173">Online Analytical Processing (OLAP) data stores</span></span>](./online-analytical-processing.md#olap-in-azure)
- [<span data-ttu-id="d9fd7-174">データ ウェアハウス</span><span class="sxs-lookup"><span data-stu-id="d9fd7-174">Data warehouses</span></span>](./data-warehousing.md)
- [<span data-ttu-id="d9fd7-175">パイプライン オーケストレーション</span><span class="sxs-lookup"><span data-stu-id="d9fd7-175">Pipeline orchestration</span></span>](../technology-choices/pipeline-orchestration-data-movement.md)

## <a name="next-steps"></a><span data-ttu-id="d9fd7-176">次の手順</span><span class="sxs-lookup"><span data-stu-id="d9fd7-176">Next steps</span></span>

<span data-ttu-id="d9fd7-177">次の参照アーキテクチャでは、Azure でのエンド ツー エンド ELT のパイプラインを示します。</span><span class="sxs-lookup"><span data-stu-id="d9fd7-177">The following reference architectures show end-to-end ELT pipelines on Azure:</span></span>

- [<span data-ttu-id="d9fd7-178">SQL Data Warehouse を使用した Azure のエンタープライズ向け BI</span><span class="sxs-lookup"><span data-stu-id="d9fd7-178">Enterprise BI in Azure with SQL Data Warehouse</span></span>](../../reference-architectures/data/enterprise-bi-sqldw.md)
- [<span data-ttu-id="d9fd7-179">SQL Data Warehouse と Azure Data Factory を使用したエンタープライズ BI の自動化</span><span class="sxs-lookup"><span data-stu-id="d9fd7-179">Automated enterprise BI with SQL Data Warehouse and Azure Data Factory</span></span>](../../reference-architectures/data/enterprise-bi-adf.md)